# Task04: 大模型的数据

对于语言模型来说，训练数据就是“原始文本”，因为语言模型建模的是 token 序列出现的联合概率。

## 大语言模型背后的数据

| 数据集 | 大小 | 模型 | 说明 |
| :---: | :---: | :---: | :---: |
| WebText | 40 G | GPT-2 | 选择 Common Crawl 一小部分，抓取至少获得3个赞的所有外链，过滤掉维基百科 |
| C4 | 806 G | T5 | 从 Common Crawl 移除了“bad words”，移除了代码（“{”），过滤掉了非英语文本 |
| GPT-3 | 45 T | GPT-3 | Common Crawl + WebText2 + Books1 + Books2 + Wikipedia |
| Pile | 825 G | GPT-2Pile | 高质量数据集训练小模型达到大模型性能，强调数据质量十分重要 |

## 数据污染问题

一般的机器学习任务中，都会保证训练数据与测试数据分离（称之为数据卫生），但对于 LLM 来说，训练数据和基准数据都来自互联网，要保证分离十分困难。所以， LLM 与小模型在基准数据集上对比性能时其实很少考虑到大模型数据泄露的问题。

在基准数据集上对比大模型的性能其实还是有失偏颇的，不过这点其实并没有那么重要，我们通常会在多个维度不同能力上进行模型性能的对比。再者说，高考碰到练习题的原题，那也是我平常做题多，你没做过原题的话，那谁让你做题少呢。
