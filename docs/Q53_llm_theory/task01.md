# Task01：引言篇

## 什么是语言模型

1. 语言模型（LM）的经典定义是一种对词元（token）序列的概率分布，即衡量了每个句子出现的概率，或者说“好坏”。
2. 这隐含着模型具备语言能力和世界知识。
3. 将序列 $x_{1:L}$的联合分布 $p(x_{1:L})$的常见写法是使用概率的链式法则：$p(x_{1:L}) = p(x_1) p(x_2 \mid x_1) p(x_3 \mid x_1, x_2) \cdots p(x_L \mid x_{1:L-1}) = \prod_{i=1}^L p(x_i \mid x_{1:i-1})$
4. 在自回归语言模型  $p$  中生成整个序列 $x_{1:L}$ ，我们需要一次生成一个令牌(token)，该令牌基于之前以生成的令牌进行计算获得：
$\begin{aligned}
\text { for } i & =1, \ldots, L: \\
x_i & \sim p\left(x_i \mid x_{1: i-1}\right)^{1 / T},
\end{aligned}$
5.  $T$用来控制语言模型生成时的变异量。

## 大模型相关历史回顾

### 信息论、熵、n-gram
1. 熵：$H(p) = \sum_x p(x) \log \frac{1}{p(x)}$,熵实际上是一个衡量将样本$x∼p$ 编码（即压缩）成比特串所需要的预期比特数的度量。
2. 交叉熵：$H(p,q) = \sum_x p(x) \log \frac{1}{q(x)}$
3. N-gram模型。在一个n-gram模型中，关于$x_{i}$的预测只依赖于最后的$n-1$个字符$x_{i−(n−1):i−1}$，而不是整个历史：$p(x_i \mid x_{1:i-1}) = p(x_i \mid x_{i-(n-1):i-1})$。
如果n太小，无法捕获长程依赖，n太大的话难以计算概率。

### 神经语言模型
1. 概率的估计通过神经网络来进行。
2. 仍然没有跳出n-gram的限制。
3. 统计上是高效的，计算上是低效的。但是随着时间推移，训练大型网络变得可行，神经语言模型已经成为主导。